{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Data\n",
    "\n",
    "Data is loaded from each respective .csv file using panda's read_csv() function. Each dataset has all data except for text and sentiment label dropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load datasets\n",
    "# movie dataset\n",
    "mov_data = pd.read_csv('data/movie.csv', delimiter=',', quotechar='\"', encoding='utf-8', on_bad_lines='skip')\n",
    "mov_data = mov_data.to_numpy()\n",
    "mov_data.shape\n",
    "mov_X = mov_data[1:, 0] # all rows except the first one, text column\n",
    "mov_y = mov_data[1:, 1] # all rows except the first one, label column\n",
    "\n",
    "# chat gpt dataset\n",
    "cgpt_data = pd.read_csv('data/chatgpt_sentiment_analysis.csv', delimiter=',', quotechar='\"', encoding='utf-8', on_bad_lines='skip')\n",
    "cgpt_data = cgpt_data.to_numpy()\n",
    "cgpt_data.shape\n",
    "cgpt_X = cgpt_data[1:, 1] # all rows except the first one, text column\n",
    "cgpt_y = cgpt_data[1:, 2] # all rows except the first one, label column\n",
    "\n",
    "# social media dataset\n",
    "sm_data = pd.read_csv('data/soc_med_sentiment_analysis.csv', delimiter=',', quotechar='\"', encoding='utf-8', on_bad_lines='skip')\n",
    "sm_data = sm_data.to_numpy()\n",
    "sm_data.shape\n",
    "sm_X = sm_data[1:, 4] # all rows except the first one, text column\n",
    "sm_y = sm_data[1:, 5] # all rows except the first one, label column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining and Vectorization\n",
    "The X dataframes, representing strings of text, were concatenated together so all strings across the three datasets were in one location.\n",
    "Due to the fact that neural models cannot read strings of text on their own, this data then  had to be vectorized. This process, being out of the scope of this assignment to accomplish, was done using sklearn's TfidfVectorizer() object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combine X and Y dataframes, then vectorize\n",
    "X = np.concatenate([mov_X, cgpt_X, sm_X])\n",
    "y = np.concatenate([mov_y, cgpt_y, sm_y])\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=1000)\n",
    "X = vectorizer.fit_transform(X).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label conversion\n",
    "Y values from the datasets, representing sentiments, were converted to numerical representations to simplify calculations and create uniformity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert labels in y\n",
    "y = np.where(y == \"good\", 1, np.where(y == \"neutral\", 0, np.where(y == \"bad\", -1, y)))\n",
    "y = np.where(y == \"positive\", 1, np.where(y == \"neutral\", 0, np.where(y == \"negative\", -1, y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "Data is shuffled, halved, and then one half of the data is split into the training (80%) and testing (20%) arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocessing\n",
    "def preprocess(X, y):\n",
    "    # Normalize the data\n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "\n",
    "    # Shuffle the data\n",
    "    indices = []\n",
    "\n",
    "    for i in range(len(X)):\n",
    "        indices.append(i)\n",
    "\n",
    "    np.random.shuffle(indices)\n",
    "    X = X[indices]\n",
    "    y = y[indices]\n",
    "\n",
    "    #Half the amount of data\n",
    "    split = int(0.5 * X.shape[0])\n",
    "    X = X[:split]\n",
    "    y = y[:split]\n",
    "\n",
    "    # Split data: training 80%, testing 20%\n",
    "    split = int(0.8 * X.shape[0])\n",
    "    X_train, X_test = X[:split], X[split:]\n",
    "    y_train, y_test = y[:split], y[split:]\n",
    "\n",
    "    # return X, X_train, X_test, y, y_train, y_test\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "X_train, X_test, y_train, y_test = preprocess(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model\n",
    "class BackPropagation:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        # Initialize weights and biases\n",
    "        self.b1 = np.zeros((1, self.hidden_size))\n",
    "        self.b2 = np.zeros((1, self.output_size))\n",
    "        self.W1 = np.random.randn(self.input_size, self.hidden_size) * np.sqrt(2 / self.input_size)\n",
    "        self.W2 = np.random.randn(self.hidden_size, self.output_size) * np.sqrt(2 / self.hidden_size)\n",
    "\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def sigmoid_derivative(self, x):\n",
    "        return x * (1 - x)\n",
    "\n",
    "    def relu(self, x):\n",
    "        return x * (x >= 0)\n",
    "\n",
    "    def relu_derivative(self, x):\n",
    "        return 1. * (x > 0)\n",
    "\n",
    "    def forward(self, X):\n",
    "        # Hidden layer\n",
    "        hidden_activation = self.relu(np.dot(X, self.W1) + self.b1)\n",
    "        # Output layer\n",
    "        output = np.dot(hidden_activation, self.W2) + self.b2\n",
    "        return output\n",
    "\n",
    "    def backward(self, X, y, output, rate):\n",
    "        # Calculate error (output)\n",
    "        output_error = (y - output)\n",
    "\n",
    "        # Calculate hidden activations\n",
    "        hidden = self.relu(np.dot(X, self.W1) + self.b1)\n",
    "        print(f\"Hidden max: {np.max(hidden)}, min: {np.min(hidden)}\")\n",
    "\n",
    "        # Calculate error (hidden)\n",
    "        hidden_error = np.dot(output_error, self.W2.T)\n",
    "        hidden_delta = hidden_error * self.relu_derivative(hidden)\n",
    "        print(f\"Hidden error max: {np.max(hidden_error)}, min: {np.min(hidden_error)}\")\n",
    "        print(f\"Hidden delta max: {np.max(hidden_delta)}, min: {np.min(hidden_delta)}\")\n",
    "\n",
    "        # Update weights\n",
    "        self.W2 = self.W2 + rate*(np.dot(hidden.T, output_error))\n",
    "        self.b2 = self.b2 + rate*(np.sum(output_error, axis=0, keepdims=True))\n",
    "\n",
    "        self.W1 = self.W1 + rate*(np.dot(X.T, hidden_delta))\n",
    "        self.b1 = self.b1 + rate*(np.sum(hidden_delta, axis=0, keepdims=True))\n",
    "        \n",
    "    def train(self, X, y, learning_rate, epochs):\n",
    "        y = y.reshape(-1,1)\n",
    "\n",
    "        for _ in range(epochs):\n",
    "            print(f\"W1 max: {np.max(self.W1)}, min: {np.min(self.W1)}\")\n",
    "            print(f\"W2 max: {np.max(self.W2)}, min: {np.min(self.W2)}\")\n",
    "            print(f\"B1 max: {np.max(self.b1)}, min: {np.min(self.b1)}\")\n",
    "            print(f\"B2 max: {np.max(self.b2)}, min: {np.min(self.b2)}\")\n",
    "            output = self.forward(X)\n",
    "            self.backward(X,y, output, learning_rate)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.forward(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "Parameters are set and a backpropagation model is initialized to train on x and y training datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 max: 0.20109642545871534, min: -0.19136706661325897\n",
      "W2 max: 0.6422843975124137, min: -0.5976149131603825\n",
      "B1 max: 0.0, min: 0.0\n",
      "B2 max: 0.0, min: 0.0\n",
      "Hidden max: 10.508063788641238, min: -0.0\n",
      "Hidden error max: 6.81963663586679, min: -6.345345724905494\n",
      "Hidden delta max: 4.550848599986745, min: -6.345345724905494\n",
      "W1 max: 13.714121562929698, min: -10.562502356067185\n",
      "W2 max: 61.05811150497766, min: -25.740352584404736\n",
      "B1 max: 1.4731273942757246, min: -17.7656700251416\n",
      "B2 max: 20.437176272959924, min: 20.437176272959924\n",
      "Hidden max: 914.6188442423286, min: -0.0\n",
      "Hidden error max: 1971189.6849890135, min: -4675814.722772738\n",
      "Hidden delta max: 405681.7539953504, min: -4675814.722772738\n",
      "W1 max: 45930970.65919723, min: -79086371.72587079\n",
      "W2 max: 129085059.31170666, min: -291868197.6792114\n",
      "B1 max: 3359550.0942404876, min: -103745340.41492735\n",
      "B2 max: -1425929.7283347908, min: -1425929.7283347908\n",
      "Hidden max: 3513411127.4536915, min: -0.0\n",
      "Hidden error max: 4.5912482530808976e+26, min: -1.0381056954768907e+27\n",
      "Hidden delta max: 1.3305051172917425e+26, min: -1.0381056954768907e+27\n",
      "W1 max: 9.524446144208121e+27, min: -1.4071484656393766e+28\n",
      "W2 max: 6.586789570444989e+28, min: -1.6449929180852967e+26\n",
      "B1 max: 1.3001510125238694e+27, min: -1.0288775407106776e+28\n",
      "B2 max: 3.389572082952301e+19, min: 3.389572082952301e+19\n",
      "Hidden max: 3.376338875941882e+29, min: -0.0\n",
      "Hidden error max: 1.2767483776874133e+85, min: -5.112285168997884e+87\n",
      "Hidden delta max: 7.2529240400938115e+84, min: -5.112285168997884e+87\n"
     ]
    }
   ],
   "source": [
    "#backprop training\n",
    "input_size = X_train.shape[1]\n",
    "hidden_size = 20\n",
    "output_size = 1\n",
    "\n",
    "bp = BackPropagation(input_size, hidden_size, output_size)\n",
    "bp.train(X_train, y_train, learning_rate=0.001, epochs=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions\n",
    "Final predictions are made on both the training and testing datasets to make comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predictions\n",
    "train_predictions = bp.predict(X_train)\n",
    "test_predictions = bp.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "Predictions are flattened to align with the size of y-value arrays and then compared using mean squared error. These results are printed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_predictions = train_predictions.flatten()\n",
    "test_predictions = test_predictions.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "OverflowError",
     "evalue": "(34, 'Result too large')",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOverflowError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m train_mse \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(\u001b[43m(\u001b[49m\u001b[43mtrain_predictions\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m)\n\u001b[0;32m      2\u001b[0m test_mse \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean((test_predictions \u001b[38;5;241m-\u001b[39m y_test) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain MSE: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_mse\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mOverflowError\u001b[0m: (34, 'Result too large')"
     ]
    }
   ],
   "source": [
    "train_mse = np.mean((train_predictions - y_train) ** 2)\n",
    "test_mse = np.mean((test_predictions - y_test) ** 2)\n",
    "\n",
    "print(f\"Train MSE: {train_mse}\")\n",
    "print(f\"Test MSE: {test_mse}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
